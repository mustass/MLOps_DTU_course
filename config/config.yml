experiment_name: monki-see-monki-sleep # This is done so that we can always find models and their corresponding datasets

data:
  train_val_test_splits: [0.6,0.2,0.2]
  max_seq_length: 3 # Max review string length for the Tokenizer
  used_datasets: { # Which dataset do we want to join and load (I can't see a case where these are not the same)
                  "Books": 0,
                  "Electronics": 0,
                  "Movies_and_TV": 0,
                  "CDs_and_Vinyl": 0,
                  "Clothing_Shoes_and_Jewelry": 0,
                  "Home_and_Kitchen": 0,
                  "Kindle_Store": 0,
                  "Sports_and_Outdoors": 0,
                  "Cell_Phones_and_Accessories": 0,
                  "Health_and_Personal_Care": 0,
                  "Toys_and_Games": 0,
                  "Video_Games": 0,
                  "Tools_and_Home_Improvement": 0,
                  "Beauty": 0,
                  "Apps_for_Android": 0,
                  "Office_Products": 0,
                  "Pet_Supplies": 0,
                  "Automotive": 0,
                  "Grocery_and_Gourmet_Food": 0,
                  "Patio_Lawn_and_Garden": 0,
                  "Baby": 0,
                  "Digital_Music": 0,
                  "Musical_Instruments": 1,
                  "Amazon_Instant_Video": 1
                  }

seed: 7

gpus: 0 # Choose -1 for all available

training:
  batch_size: 64     # Batch size
  lr: 0.02           # Learning rate
  max_epochs: 1
  full: false        # Whether we train the whole BART architecture or only our outer layers
  num_workers: 4

prediction:

compute:
    cloud: true # if cloud is false, all the rest is ignored. If clous is true is provided, they are used.
    subscription_id: "aa4b9a75-08b4-4169-a974-a772bd186a7a"
    resource_group: "mlops_day6"
    workspace_name: "mlops_project"
    workspace_exists: true
    location: "westeurope" # ignored if workspace_exists is true
    compute_target: "s2105271"
    environment_name: "myEnvironment"
    deploy: true
  
wandbkey: "33f3c65ab2fd849f6c70cc9f40a93f6b63bf7243"

hp:
  'tune': False
  'lr_low': 0.001
  'lr_high': 0.1
  'author': asgermunch
  'project': transformers-group-octupus-src_models
