data:
  raw_data_path: './data/raw/AmazonProductReviews.csv' # Where we save and load raw data from
  train_val_test_splits: [0.6,0.2,0.2]
  max_seq_length: 65 # Max review string length for the Tokenizer
  used_datasets: { # Which dataset do we want to join and load (I can't see a case where these are not the same)
                  "Books": 0,
                  "Electronics": 0,
                  "Movies_and_TV": 0,
                  "CDs_and_Vinyl": 0,
                  "Clothing_Shoes_and_Jewelry": 0,
                  "Home_and_Kitchen": 0,
                  "Kindle_Store": 0,
                  "Sports_and_Outdoors": 0,
                  "Cell_Phones_and_Accessories": 0,
                  "Health_and_Personal_Care": 0,
                  "Toys_and_Games": 0,
                  "Video_Games": 0,
                  "Tools_and_Home_Improvement": 0,
                  "Beauty": 0,
                  "Apps_for_Android": 0,
                  "Office_Products": 0,
                  "Pet_Supplies": 0,
                  "Automotive": 1,
                  "Grocery_and_Gourmet_Food": 0,
                  "Patio_Lawn_and_Garden": 1,
                  "Baby": 0,
                  "Digital_Music": 0,
                  "Musical_Instruments": 1,
                  "Amazon_Instant_Video": 0
                  }

seed: 7

training:
  batch_size: 64     # Batch size

compute:
  azure: { # if local is provided, all this is ignored. If azure is provided, they are used.
    subscription_id: "dcb1598f-693c-460c-8648-667de51a2475",
    resource_group: "myResourceGroup",
    workspace_name: "s202798Workspace",
    compute_target: "s202798Compute",
    environment_name: "CoolName",
    experiment_name: "ExperimentName",
    location: "northeurope"
  }